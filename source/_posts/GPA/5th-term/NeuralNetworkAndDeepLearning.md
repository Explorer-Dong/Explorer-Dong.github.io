---
title: NeuralNetworkAndDeepLearning
categories:
  - GPA
  - 5th-term
category_bar: true
---

## 神经网络与深度学习

## 前言

学科地位：

| 主讲教师 | 学分配额 | 学科类别 |
| :------: | :------: | :------: |
|   宋歌   |    3     |  自发课  |

成绩组成：

| 小组作业 | 个人大作业 |
| :------: | :--------: |
|   40%    |    60%     |

教材情况：

|      课程名称      |        选用教材        | 版次 |  作者  |     出版社     |      ISBN 号       |
| :----------------: | :--------------------: | :--: | :----: | :------------: | :---------------: |
| 神经网络与深度学习 | 《神经网络与深度学习》 |  1   | 邱锡鹏 | 机械工业出版社 | 978-7-111-64968-7 |

学习资源：

- 课程官网：https://nndl.github.io/

为什么要学这门课？

> 还记得 NJU 的 jyy 老师在上 OS 时说过的一句话，“我们现在学习的微积分是 300 年前的人类智慧结晶，何不再学学 50 年前的人类智慧结晶呢？”印象深刻。当一切都可以用层层嵌套的简单函数模拟时，人类社会必将发生翻天覆地的变化！

会收获什么？

>如何学习特征？如何优化调参？为什么要这样做？背后的原理是什么？

## 1 绪论

**表示学习是什么**？与传统的特征工程目的一致，为了得到数据中的更好的特征。不同的是，特征工程中的策略都是可控的方式，而表示学习就是利用深度学习从数据中学习高层的有效特征。

**深度学习是什么**？我们知道机器学习就是在手动处理完特征后，构建对应的模型 **预测输出**。而深度学习就是将机器学习的手动特征工程也用模型进行 **表示学习** 来学习出有效特征，然后继续构建模型 **预测输出**。如下图所示：

![深度学习的数据处理流程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202409121508150.png)

**为什么会有深度学习**？最简单的一点就是，很多特征我们根本没法定义一种表示规则来表示特征，比如说对于图像，怎么定义复杂的图像的特征呢？比如说对于音频，又怎么定义复杂的音频的特征呢？没办法，我们直接学特征！

**神经网络是什么**？就是万千模型中的一种，仅此而已。

**为什么用神经网络进行深度学习**？有了上面对深度学习定义的理解，可以发现其中最具有挑战性的特点就是，模型怎么知道什么才是好特征？什么是不好的特征？神经网络可以很好的解决这个问题。通过由浅到深层层神经元的特征提取，越深的神经元就可以学习更高语义的特征。说的高大上一点就是，神经网络可以很好的解决深度学习中的「贡献度分配」问题。

**机器学习流程**。数据 $\to$ 模型 $\to$ 学习准则 $\to$ 优化算法

**线性模型**。学习任务是分类、回归。学习准则有：

- 经验风险最小化：通过 **最小化训练集上的损失函数** 来学习模型参数。例如：线性回归中的最小二乘损失。
- 结构风险最小化：引入 **正则化** 来控制模型的复杂度，避免过拟合。
- 最大似然估计：通过 **最大化模型在给定数据上的似然函数** 来估计模型参数。例如二分类 logistic 和多分类 softmax 中的交叉熵损失。
- 最大后验估计：结合了似然函数和 **先验** 分布，在贝叶斯框架下估计模型参数。

**优化算法**。最小二乘法、梯度下降法、拟牛顿法。

## 2 前馈神经网络

所谓的前馈神经网络，就是网络模型中的全连接层。通过向前计算数据，向后更新参数实现拟合的功能。

前馈神经网络模型的学习准则一般采用交叉熵损失，优化算法一般采用小批量随机梯度下降 (Mini-batch Stochastic Gradient Descent, 简称 Mini-batch SGD)。

### 2.1 神经元

**网络模型中的最小学习单元是什么**？神经元。每一个神经元接受输入 $z$，通过设定好的激活函数 $f$，给出输出 $a=f(z)$。神经元中的激活函数大致种类极多，主要有以下 3 种：

- Sigmoid 函数。例如 logistic 函数和 Tanh 函数。
- Relu 函数。
- 复合函数。

**激活函数的 4 个原则是什么？为什么**？。非线性、可导、单调、有界。为了确保网络可以拟合复杂的映射关系，需要激活函数是非线性的；为了便于对网络求导从而进行参数更新，需要确保激活函数是可导的；为了防止对网络求导的过程中出现梯度消失或者梯度爆炸，需要确保激活函数是单调并且有界。

### 2.2 反向传播算法

注：我们定义输入层为前，输出层为后。

在进行随机梯度下降时。经过简单的推导可以发现，第 $l$ 层的损失 $\delta (l)$ 依赖于后一项的损失 $\delta(l+1)$，于是每一个样本更新参数的逻辑就是从输出层开始逐层往前直到第一层隐藏层进行更新。

## 3 卷积神经网络

## 4 循环神经网络

## 5 记忆与注意力机制

## 6 网络优化与正则化

## 7 概率图模型

## 8 深度生成模型

## 9 深度强化学习

## 个人大作业 *

1. 背景介绍：做好国内外研究现状的调研。
2. 项目理解：深入对模型与代码的理解。
3. 充分实验：对比实验、验证实验（消融实验）、参数实验（验证模型对不同的参数的敏感性）。
4. 项目总结。
