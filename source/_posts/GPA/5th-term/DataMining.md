---
title: DataMining
categories:
  - GPA
  - 5th-term
category_bar: true
---

## 数据挖掘

## 前言

学科地位：

| 主讲教师 | 学分配额 | 学科类别 |
| :------: | :------: | :------: |
|  郑智超  |   3+1    |  专业课  |

成绩组成：

|   理论课    |      |   实验课    |      |
| :---------: | :--: | :---------: | :--: |
|    考勤     |  5%  |    考勤     | 10%  |
| 作业 (书面) | 20%  | 作业 (编程) | 30%  |
| 期末 (闭卷) | 75%  |   大作业    | 60%  |

教材情况：

| 课程名称 |         选用教材         | 版次 |         作者         |     出版社     |      ISBN 号       |
| :------: | :----------------------: | :--: | :------------------: | :------------: | :---------------: |
| 数据挖掘 | 《数据挖掘：原理与应用》 |  1   | 丁兆云，周鋆，杜振国 | 机械工业出版社 | 978-7-111-69630-8 |

为什么要学这门课？

> 数据量在进入信息化时代后指数级增长，显然这些数据一定具有某种价值。之前看到过这样的说法 “你能想到和做到的，前人都已经想到和做到了”。虽然这样的说法不具有绝对性，但是个人认为对于大多数人都是适用的。也就是说，人类的行为甚至是万物的变迁都是符合统计学规律的，而这些规律都藏在浩瀚的数据之中。
>
> 至于这门课安排在此的意义，个人认为是对大二时学习机器学习的一个补充。更加详细的介绍了无监督学习的各个算法。

会收获什么？

> 数据预处理的基本策略和一些无监督学习算法。

## 1 绪论

**科学发展范式**

实验（经验）科学 $\to$ 理论科学 $\to$ 计算科学 $\to$ 数据科学。

**属性分类**

- 定性。二元属性、标称属性、序数属性；
- 定量。即数值属性，可以用合适的 **统计量** 进行描述。

**统计量**

- 中心趋势度量。用来描述数据集中心位置的统计量，它反映了数据的平均水平或典型值。例如：
    1. 算术平均数（受计算数据的影响大）；
    2. 调和平均数（特定场景）；
    3. 中位数（适用于序数申诉信，表示位置信息，不受极差影响）；
    4. 众数（不受极差影响）。
- 离散度度量。用来描述数据分布的广泛程度，即数据值偏离其中心趋势的程度。例如：
    1. 极差（适用于数据极端值较少且分布不复杂的场景）；
    2. 标准差（解释性比方差更好，反应数据与均值之间的关系，对极端值敏感）；
    3. 四分位数间距（反应数据内部的离散程度，容易忽略极端数据）。

**可视化策略**

- 箱型图、五数概括、直方图。有助于可视化 **单个属性** 的数据分布；
- 饼图。有助于可视化 **单个属性** 的数据占比；
- 散点图。有助于可视化 **两个属性** 的相关关系。

**邻近性度量**

所谓邻近性度量，可以简单的将其理解为计算任意两个数据对象之间的距离。由于数据对象的属性有很多种类，不同种类的邻近性度量方法不同，因此当我们在对「两个数据对象」进行邻近性度量时，是「根据属性类别分组计算」的。下面讲一下不同属性类别的邻近性度量方法：

- 标称属性。我们在实际计算两个数据对象标称属性的相似性或相异性时，一般为了便于计算需要对其进行编码，然后再比较两个数据对象的所有标称属性中 **相同或相异的编码个数占总标称属性的数量**。常见的标称属性有：籍贯、学历等等；

- 二元属性。与标称属性类似同样需要进行编码，但会取决于二元属性的对称性。对于两个数据对象 x_i 和 x_j，在二元属性上的取值情况为：都取 1 的数量为 m，都取 0 的数量为 q，取 (1,0) 的数量为 n，取 (0,1) 的数量为 p。如下图所示：

    ![两个数据对象的二元属性取值情况](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202412031120688.png)

    则有：

    - 对称二元属性。相异性为 $\displaystyle \frac{p+n}{m+n+p+q}$，相似性就是 $1-\text{相异性}$。常见的对称二元属性有：朋友、配偶；

    - 非对称二元属性。相异性为 $\displaystyle \frac{p+n}{m+n+p}$，相似性就是 $1-\text{相异性}$。常见的非对称二元属性有：上下级、父母与子女、教师与学生；

- 数值属性。对于两个数值型的数据对象 $\boldsymbol x$ 和 $\boldsymbol y$，定义了一下两种邻近性度量指标：

    - 闵可夫斯基距离。取值范围为 $[0,+\infty]$。$h = 1$ 为曼哈顿距离，$h = 2$ 为欧氏距离，$h = \infty$ 为切比雪夫距离。如下式：
        $$
        \begin{aligned}
        \text{闵可夫斯基距离：}& d(\boldsymbol x,\boldsymbol y) = \| \boldsymbol x - \boldsymbol y \|_h \\
        \text{曼哈顿距离：}& d(\boldsymbol x,\boldsymbol y) = \| \boldsymbol x - \boldsymbol y \|_1 \\
        \text{欧几里得距离：}& d(\boldsymbol x,\boldsymbol y) = \| \boldsymbol x - \boldsymbol y \|_2 \\
        \text{切比雪夫距离：}& d(\boldsymbol x,\boldsymbol y) = \| \boldsymbol x - \boldsymbol y \|_\infty = \max(\boldsymbol x - \boldsymbol y)
        \end{aligned}
        $$

    - 余弦距离。取值范围为 $[0, 2]$。就是两个数据对象的余弦值的相反数加 1 (这很巧妙的利用了余弦值进行了相似性度量并且还确保了计算结果的非负性)。这个度量方法偏向于语言上描述两个对象的差异性，并不适合进行度量测量 (metric measure)，因为其不具备三角不等式关系。如下式：
        $$
        d(\boldsymbol x,\boldsymbol y) = 1 - \cos < \boldsymbol x,\boldsymbol y>
        $$

- 序数属性。使用排名法对属性的每一个可能的取值进行编码，然后归一化到 $[0,1]$ 范围内，最后就可以使用上面的数值属性的方法进行两个数据对象的邻近性度量。

- 混合属性。如果出现了不止一种上述类别的属性，可以采用 **加权平均** 的方式进行邻近性度量。

## 2 数据预处理

数据决定上限，模型和算法只是尽可能逼近这个上限，因此数据的质量是核心所在。数据的质量可以被描述为这几个方面：准确性、完整性、一致性、合时性、可信度、解释性。

接下来我们主要介绍数据预处理的几个主要任务，将数据处理成模型可接受的形式并且提升数据的质量。

### 2.1 数据清理

大约需要处理两类问题：缺失值处理、噪声处理。

关于缺失值处理。

- 显然我们可以直接删除含有缺失属性的元组。
- 也可以进行缺失值填充。填充策略比较多，简单点可以用平均数、众数等策略进行填充，也可以用前推法、后推法、插值法、滑窗均值法等策略进行填充，还可以用贝叶斯等概率策略进行填充。

关于噪声处理。

- 可以用箱型图去掉离群点。

- 也可以对数据做平滑处理。平滑策略有很多，例如：函数拟合平滑、近邻局部平滑、指数平滑。函数拟合平滑比较显然，就是利用线性回归拟合一个函数即可，下面展开后两者策略的简介：

    - 近邻局部平滑（分箱法）。我们将原始数据排序后，通过「等深分箱（按元素数量）」或「等宽分箱（按元素值域）」或「自定义分箱」等策略划分为不同的子集，然后对每一个子集进行平滑处理，子集可以用均值、中位数、边界、窗口均值等策略做平滑处理。

    - 指数平滑（递推法）。定义权重参数 $\alpha \in (0,1)$，原始数据记作 $x_1,x_2,...,x_n$，平滑后数据记作 $s_1,s_2,...,s_n$，则有：
        $$
        s_i = \alpha x_i + (1 - \alpha) s_{i-1}
        $$

### 2.2 数据集成

借鉴数据库的逻辑，将多源数据通过外键集成到一个完整的数据集合中。可以理解为升维。

### 2.3 数据规约

所谓数据规约就是降低数据集的规模。大约有三种策略分别为：降维、降数据、数据压缩。其中：

- 降维可以采用对应的机器学习策略。参考之前的笔记 <https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#特征映射>
- 降数据可以采用简单随机抽样或者分层抽样。
- 数据压缩就是从编码角度进行数据编码和数据压缩。

### 2.4 数据规范化 | 离散化

数据规范化就是所谓的「归一化」方法，从而避免不同属性的量纲之间的影响。常用的有：

- 最大最小规范化。规范公式为 $\displaystyle x'= \frac{x - x_{\min}}{x_{\max} - x_{\min}}(r - l) + l$。缺点是容易受离群点的影响，且测试数据可能会超过阈值 $[l,r]$。
- 均值标准差规范化。规范公式为 $\displaystyle x'=\frac{x - \overline{x}}{\sigma_x}$。缺点是计算量相对更大。
- 零均值规范化。规范公式为 $\displaystyle x'=\frac{x}{10^j}$，其中 $j$ 为使得 $\displaystyle \frac{\max{|x|}}{10^j}<1$ 的最小取值。

数据离散化就是对连续属性值的属性进行离散化操作。从而适用于只能处理离散属性的场景。

## 3 关联分析

本章我们学习一种无监督学习方法：关联分析。

### 3.1 基本概念

![关联向量数据集](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202410210822548.png)

基本的定义。首先定义每一列的字段为 **项** Item，项的集合就叫做 **项集** Itemset。每一行表示一个 **事物** tuple，显然事物是一个项集的实例。如果我们定义项集 $X$，所有事物中包含 $X$ 的个数就叫做 $X$ 的 **支持度** $\sigma (X)$，若 $\sigma (X)$ 超过了某个阈值，则称 $X$ 为 **频繁项集**。

规则的定义。为了知道两个不相交的项集 $X,Y\quad(X \bigcup Y=\empty)$ 之间的关联性，引入 **规则 $X\to Y$ 的支持度 $s$ 和置信度 $c$** 的概念。其中 $s(X\to Y)=\frac{\sigma(X\bigcup Y)}{N},\quad c(X\to Y)=\frac{\sigma(X\bigcup Y)}{\sigma(X)}$。从概率上来看不难发现，$s(X\to Y)=P(X\bigcup Y),\quad c(X\to Y)=P(Y|X)$。若 $X$ 和 $Y$ 的支持度和置信度同时超过了对应的阈值，则称 $X$ 和 $Y$ 之间的关联是 **强规则** 的。

而关联分析的任务就是在给定事物集、支持度阈值、置信度阈值的情况下，找到所有的强规则。

### 3.2 支持度检测算法

显然可以直接枚举 $X\to Y$ 的所有项集然后进行支持度检测和置信度检测，但是这样的时间复杂度过高。常规的优化算法就是支持度检测和置信度检测分模块串行计算。先用支持度检测算法寻找出所有的频繁项集，然后再进行支持度检测。下面介绍两个经典的支持度检测算法来寻找出所有的频繁项集。

#### 3.2.1 Apriori 算法

项集的搜索空间如下示例所示：

![项集的搜索空间 - 示例](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202410210925596.png)

为了降低枚举出来的项集的个数以及重复性，Apriori 算法的核心思路就是「非频繁项集的超集一定也是非频繁项集」，对应的有两个优化步骤：

- 自连接。在 **利用 $k$ 项集构造 $k+1$ 项集** 时。用所有的 $k$ 频繁项集自连接来构造 $k+1$ 项集。两个 $k$ 频繁项集可以连接的前提是它们有 $k-1$ 项是相同的
- 剪枝。在 **降低搜索空间** 时。如果当前项集是不频繁的，那么搜索空间中当前项集的超集都直接被减掉。

当然，在进行支持度计算时，涉及到了向量匹配算法，可以用字典树进行常数优化。这里被称为哈希树。

需要注意的是，由于计算置信度时需要使用之前计算过的支持度信息，当频繁项集很多时，这会造成很大的存储消耗，为了减少这种存储消耗，我们需要减少频繁项集的数量，为此引入「极大频繁项集」和「闭频繁项集」来压缩频繁项集的数量从而在保留重要信息的前提下减少存储消耗。其中极大频繁项集定义为「某个频繁项集的直接超集都不是频繁项集」，而闭频繁项集定义为「某个频繁项集的直接超集与它的支持度计数都不一样」。

这里再补充一个 Apriori 算法的变种，叫做 Eclat 算法。其算法逻辑是将项集编号和项集进行「反拉链」存储，后续在统计频繁项集时只需要取交集即可，如下图所示：

![Eclat 算法示例 假设最小支持度阈值为 2](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202411110820401.png)

但是即使上面这么折腾进行优化，Apriori 的计算复杂度仍然很高。因为连接步产生的候选项集仍然可能有有很多，并且对于每一个候选项集，即使使用哈希树也几乎需要遍历一遍事务集。下面介绍 FP-Growth 算法。

#### 3.2.2 FP-Growth 算法

这其实就是 trie 树的弱化版。显然 trie 可以拆项并计数，由于事务集的每一个事务中的项的顺序是无关的，因此在构造 trie 树之前可以自定义项的顺序，常规方法就是对每一个事务中的项按照其出现的频率降序排序（这里借鉴了哈夫曼树的构造策略，但也不一定会简化树的结构）。接下来构造一个 trie 树即可。当然，在一开始对项的出现频率排序时，可以提前删除不符合的候选一项集。

从上面的算法描述可以看出，我们需要对事务集扫描两边来维护出必要的信息。举例说明：假设最小支持度阈值为 22.2%。

一）首先扫描一遍事务集并将项集按照某种规则排序（一般是按照频率降序排序）

![按照频率降序排序](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202411041037092.png)

二）接着扫描一遍事务集构造 FP 树

![构造 FP 树](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202411041037337.png)

构造完 FP 树后，如何寻找频繁项集呢？从候选一项集（叶子结点）开始向上搜索合适的路径，将合适的结点排列组合即可得到频繁项集。示例代码如下：

{% fold light @FP-Growth 实现 %}

上述事务集对应输入：

```
Member_number,itemDescription
1,b
1,a
1,e
2,b
2,d
3,b
3,c
4,b
4,a
4,d
5,a
5,c
6,b
6,c
7,a
7,c
8,b
8,a
8,c
8,e
9,b
9,a
9,c
```

代码：

```python
ori_data = pd.read_csv('./ppt_demo_data.txt')

d: {tuple, list} = {}
for _, row in ori_data.iterrows():
    key = row['Member_number']
    value = row['itemDescription']
    if key in d:
        d[key].append(value)
    else:
        d[key] = [value]

data = []
for key, value in sorted(d.items()):
    data.append(value)

te = TransactionEncoder()
te_ary = te.fit(data).transform(data)
df = pd.DataFrame(te_ary, columns=te.columns_)
print(df.shape)
print(df)

frequent_itemsets: pd.DataFrame = fpgrowth(df, min_support=2/9, use_colnames=True)
print(frequent_itemsets)
```

事务集 one-hot 矩阵：

![事务集 one-hot 矩阵](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202411112028666.png)

满足 2/9 最小支持度阈值的频繁项集：

![满足 2/9 最小支持度阈值的频繁项集](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202411112028168.png)

{% endfold %}

当然，如果第一步中的排序规则不合理很容易导致建树的开销很大（几乎是指数级别），这也就会导致在一些情况下 FP-Growth 算法甚至不如 Apriori 算法。

### 3.3 置信度检测算法

有了频繁项集，接下来就是在其中寻找符合置信度的项集从而得到强关联规则。显然的，对于每一个频繁项集 V，假设 $|V|=k$，那么就可以 $2^k-2$ 次枚举所有可能的关联规则进行置信度检测。但是指数级别的时间复杂度是不被允许的，尝试优化。

不难发现，根据置信度 $c$ 的定义式 $c(X\to Y)=\frac{\sigma(X\bigcup Y)}{\sigma(X)}$，如果一个频繁项集被划分为 $\{X,Y\}$ 后不满足置信度阈值，则 $X$ 的子集对应的划分方法一定都不满足置信度阈值，以此可以对搜索空间进行剪枝来降低时间复杂度。

### 3.4 度量评估

在得到所有的强关联规则后，如何进行评估呢？下面给出一些度量评估方法。

- 提升度：$\displaystyle\text{lift}(X,Y)=\frac{P(X\cup Y)}{P(X)P(Y)}$
- 全置信度：$\displaystyle \text{all\_conf}(X, Y) = \min\{P(X|Y), P(Y|X)\}$
- 最大置信度：$\displaystyle \text{max\_conf}(X, Y) = \max\{P(X|Y), P(Y|X)\}$
- Kluc 度量：$\displaystyle \text{Kluc}(X, Y) = \frac{P(X|Y)+P(Y|X)}{2}$
- 余弦度量：$\displaystyle \text{Cosine}(X, Y) = \sqrt{P(X|Y)P(Y|X)}$

提升度的解释。对于两个项集 $X$ 和 $Y$，如果 $\frac{P(X\bigcup Y) }{P(X)P(Y)} >1$ 则说明 $X$ 和 $Y$ 是正相关的，这也是我们进行关联分析所感兴趣的；如果 $\frac{P(X\bigcup Y) }{P(X)P(Y)} = 1$ 则说明 $X$ 和 $Y$ 是相互独立的，没有研究意义；如果 $\frac{P(X\bigcup Y) }{P(X)P(Y)} < 1$ 则说明 $X$ 和 $Y$ 是负相关的，其中一项的出现会抑制另一项的出现，也没有研究意义。

### 3.5 扩展研究

当待分析的内容不是项集时，例如连续数值属性时，有必要进行连续数值离散化。

更进一步的，如果我们希望分析项集的先后时序关系，可以将频繁项集拓展为频繁序列，例如频繁时序序列分析。

## 4 聚类问题

本章再学一种无监督学习方法。不重复造轮子，详见《机器学习》课程的聚类笔记：<https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#聚类>。

## 5 异常检测

异常检测也可以称为离群点检测，但是这里的离群点检测与之前使用箱型图进行的异常检测并非一回事。这里的异常检测需要将「离群点」和「噪声」区分开来，而在非异常检测的场景下并不会将这两者区分。下面将会从「异常检测的概念」和「异常检测的算法」两个部分展开。

### 5.1 异常检测概念

由于异常检测时，异常数据的特征没有统一标准，因此很难使用二分类器将正常数据和异常数据进行区分。并且由于异常检测任务很难做到所有的数据对象都有标签（例如实时监测），因此常常使用无监督的方法进行异常检测。

### 5.2 异常检测算法

#### 孤立森林 (Isolation forest)

这是一个基于集成树模型的「**无监督**」离群点检测算法。对于森林中的每一棵树，初始时含有所有的数据对象，每次对树的所有结点进行分裂，分裂规则为选定某一个属性并计算出阈值，然后基于这个阈值将结点中的数据对象划分为两部分，直到一个结点只含有一个数据对象或者达到最大深度时停止分裂。孤立森林中树之间的区别本质上就是划分准则选择的属性顺序不同。这与决策树算法比较类似。

森林有了，如何确定异常点呢？我们定义第 $i$ 个样本 $x_i$ 的异常分数 $s$ 为下式：
$$
s(x_i,N) = \exp\left\{-\frac{E(h(x_i))}{c(N)}\right\}
$$
其中 $c(N)$ 表示数据集大小为 N 时每一个结点的平均路径长度，$E(h(x_i))$ 表示样本点 $x_i$ 在所有树中的平均路径长度。异常分数与接近 1 就表明当前样本点越有可能是异常点，越接近 0 就越不可能是异常点，如果数据集中样本点的异常分数都接近 0.5 就说明当前数据集没有异常点。

- 优点：计算效率更高；

- 缺点：不适用于高维稀疏的样本分布。

#### 自编码器 (Auto Encoder)

这是一个基于神经网络模型的「**无监督**」离群点检测算法。

- 优点：泛化能力更强；
- 缺点：由于使用了神经网络模型，因此当数据量较小时容易发生过拟合；当然了如何设计一个良好的网络模型也具有挑战性。

#### 基于邻近度的离群点检测算法

**基于距离**

**基于网格**

**基于密度**。定义数据对象 x 的 k-距离为 $dist_k(x)$ 为全体数据对象中与其相距第 k 大的距离值。定义 $N_k(x)$ 为全体数据对象中距离数据对象 x 的距离小于 k-距离的数据对象个数。于是可以得到以下两个指标：

- 数据对象 x 的局部可达密度 (local reachable density)：
    $$
    lrd_k = \frac{|N_k(x)|}{|N_k(x)| \times dist_k(x)} = \frac{1}{dist_k(x)}
    $$

- 数据对象 x 的局部离群点因子 (Local Outlier Factor)：
    $$
    LOF_k = \frac{\sum_{x'\in N_k(x)} \frac{lrd_k(x')}{lrd_k(x)} }{|N_k(x)|}
    $$
  其中：
  - 当 LOF 接近 1 时说明当前点和邻近点的密度相似，可能是正常点；
  - 当 LOF 小于 1 时说明当前点的密度比邻近点更大，可能是簇中心；
  - 当 LOF 远大于 1 时说明当前点的密度远小于邻近点的密度，可能是离群点。

## 6 分类问题

### 6.1 决策树模型

见 <https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#决策树模型>

### 6.2 邻近性模型

见 <https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#k-近邻学习>

### 6.3 时序预测

见 <https://blog.dwj601.cn/GPA/5th-term/DeepLearning/#3-循环神经网络>

### 6.4 分类结果评价

见 <https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#分类任务>
