---
title: DataMining
categories:
  - GPA
  - 5th-term
category_bar: true
---

## 数据挖掘

## 前言

学科地位：

| 主讲教师 | 学分配额 | 学科类别 |
| :------: | :------: | :------: |
|  郑智超  |   3+1    |  专业课  |

成绩组成：

|   理论课    |      |   实验课    |      |
| :---------: | :--: | :---------: | :--: |
|    考勤     |  5%  |    考勤     | 10%  |
| 作业 (书面) | 20%  | 作业 (编程) | 30%  |
| 期末 (闭卷) | 75%  |   大作业    | 60%  |

教材情况：

| 课程名称 |         选用教材         | 版次 |         作者         |     出版社     |      ISBN 号       |
| :------: | :----------------------: | :--: | :------------------: | :------------: | :---------------: |
| 数据挖掘 | 《数据挖掘：原理与应用》 |  1   | 丁兆云，周鋆，杜振国 | 机械工业出版社 | 978-7-111-69630-8 |

为什么要学这门课？

> 数据量在进入信息化时代后指数级增长, 显然这些数据一定具有某种价值. 之前看到过这样的说法 "你能想到和做到的, 前人都已经想到和做到了", 虽然这样的说法不具有绝对性, 但是个人认为对于大多数人都是适用的. 也就是说, 人类的行为甚至是万物的变迁都是符合统计学规律的, 而这些规律都藏在浩瀚的数据之中. 因此个人认为这门课对于研究统计学规律会有一些入门的帮助.

会收获什么？

> 其实, 预测模型已经有相当完善的体系了, 各种框架, 模型, 网络, 都已经经过了大量的使用与验证并且使用门槛极低. 因此这门课可能带来的收获就是「数据预处理」的本事. 如果不谈数据收集的过程, 只谈数据处理的方法和策略, 那么大概会有这样的几个过程:
>
> 1. 数据清洗： 缺失值处理, 重复值处理, 异常值处理
> 2. 数据集成： 将零散的数据集成为一个完整的数据集
> 3. 数据转换： 非数值型数据编码, 标准化/归一化, 离散化/连续化
> 4. 特征工程： 提取特征, 筛选特征
> 5. 数据采样： 过采样/欠采样
>
> 上述每一个方法与过程的背后都有很大的学问, 且学且珍惜.
>
> 当然, 如何选择模型进行参数炼丹, 也许也会收获一些策略上的经验. 等在后续学习体验的过程中再进行补充.

## 1 绪论

**科学发展范式**。实验 (经验) 科学 $\to$ 理论科学 $\to$ 计算科学 $\to$ 数据科学。

**属性分类**。定性、定量。其中定性属性可以分为三类（二元属性、标称属性、序数属性）；定量属性即数值属性，可以用合适的 **统计量** 进行描述。

**统计量**。分中心趋势度量和离散度度：

- 中心趋势度量。用来描述数据集中心位置的统计量，它反映了数据的平均水平或典型值。例如：
    1. 算术平均数（受计算数据的影响大）
    2. 调和平均数（特定场景）
    3. 中位数（适用于序数申诉信，表示位置信息，不受极差影响）
    4. 众数（不受极差影响）
- 离散度度量。用来描述数据分布的广泛程度，即数据值偏离其中心趋势的程度。例如：
    1. 极差（适用于数据极端值较少且分布不复杂的场景）
    2. 标准差（解释性比方差更好，反应数据与均值之间的关系，对极端值敏感）
    3. 四分位数间距（反应数据内部的离散程度，容易忽略极端数据）

**可视化策略**。按照数据类型有以下三种策略：

- 箱型图、五数概括、直方图。有助于可视化单个属性的分布情况
- 饼图。有助于表示单个属性的数据分布占比情况。
- 散点图。有助于可视化两个属性的相关关系。

**邻近性度量**。当我们在对「两个数据对象」进行邻近性度量计算时，是将所有同一个类别的属性联合起来计算的，例如当我们在计算二院属性的相异性或者相似性时，是将所有的二元属性联合起来一起计算的。下面讲一下不同属性类别的邻近性度量方法：

- 标称属性。我们在实际计算两个对象的标称属性的相似或相异性时，一般为了便于计算，需要进行编码。然后再比较两个数据对象的所有标称属性中相同或相异的编码个数占总标称属性数量的大小。
- 二元属性。与标称属性类似，相异性的计算就是计算 00 和 11 的二元属性取值数量，相异性计算是，如果对称属性，则为 10 和 01 的数量，如果是非对称属性，则为 10 或 01 的数量。
- 数值属性。闵可夫斯基距离（h = 1 为曼哈顿距离，h = 2 为欧氏距离，h = $\infty$ 为切比雪夫距离）、余弦距离（偏向于语言上描述两个对象的相似性，并不具备三角不等式关系）。
- 序数属性。使用排名法对属性的每一个可能的取值进行编码，然后归一化到 $[0,1]$ 范围内，最后就可以使用上面的数值属性的方法进行两个数据对象的邻近性度量。
- 混合属性。如果出现了不止一种上述类别的属性，可以采用加权平均的方式进行邻近性度量。

## 2 数据预处理

数据决定上限，模型和算法只是尽可能逼近这个上限，因此数据的质量是核心所在。数据的质量可以被描述为这几个方面：准确性、完整性、一致性、合时性、可信度、解释性。

接下来我们主要介绍数据预处理的几个主要任务，将数据处理成模型可接受的形式并且提升数据的质量。

### 2.1 数据清理

大约需要处理两类问题：缺失值处理、噪声处理。

关于缺失值处理。

- 显然我们可以直接删除含有缺失属性的元组。
- 也可以进行缺失值填充。填充策略比较多，简单点可以用平均数、众数等策略进行填充，也可以用前推法、后推法、插值法、滑窗均值法等策略进行填充，还可以用贝叶斯等概率策略进行填充。

关于噪声处理。

- 可以用箱型图去掉离群点。

- 也可以对数据做平滑处理。平滑策略有很多，例如：函数拟合平滑、近邻局部平滑、指数平滑。函数拟合平滑比较显然，就是利用线性回归拟合一个函数即可，下面展开后两者策略的简介：

    - 近邻局部平滑（分箱法）。我们将原始数据排序后，通过「等深分箱（按元素数量）」或「等宽分箱（按元素值域）」或「自定义分箱」等策略划分为不同的子集，然后对每一个子集进行平滑处理，子集可以用均值、中位数、边界、窗口均值等策略做平滑处理。

    - 指数平滑（递推法）。定义权重参数 $\alpha \in (0,1)$，原始数据记作 $x_1,x_2,...,x_n$，平滑后数据记作 $s_1,s_2,...,s_n$，则有：
        $$
        s_i = \alpha x_i + (1 - \alpha) s_{i-1}
        $$

### 2.2 数据集成

借鉴数据库的逻辑，将多源数据通过外键集成到一个完整的数据集合中。可以理解为升维。

### 2.3 数据规约

所谓数据规约就是降低数据集的规模。大约有三种策略分别为：降维、降数据、数据压缩。其中：

- 降维可以采用对应的机器学习策略。参考之前的笔记 <https://blog.dwj601.cn/GPA/4th-term/MachineLearning/#第10章-降维与度量学习>
- 降数据可以采用简单随机抽样或者分层抽样。
- 数据压缩就是从编码角度进行数据编码和数据压缩。

### 2.4 数据规范化 | 离散化

数据规范化就是所谓的「归一化」方法，从而避免不同属性的量纲之间的影响。常用的有：

- 最大最小规范化。规范公式为 $\displaystyle x'= \frac{x - x_{\min}}{x_{\max} - x_{\min}}(r - l) + l$。缺点是容易受离群点的影响，且测试数据可能会超过阈值 $[l,r]$。
- 均值标准差规范化。规范公式为 $\displaystyle x'=\frac{x - \overline{x}}{\sigma_x}$。缺点是计算量相对更大。
- 零均值规范化。规范公式为 $\displaystyle x'=\frac{x}{10^j}$，其中 $j$ 为使得 $\displaystyle \frac{\max{|x|}}{10^j}<1$ 的最小取值。

数据离散化就是对连续属性值的属性进行离散化操作。从而适用于只能处理离散属性的场景。

## 3 关联分析

### 3.1 基本概念

### 3.2 相关算法

### 3.3 度量评估

## 分类问题

## 聚类问题

## 异常检测
