---
title: MachineLearning
categories:
  - GPA
  - 4th-term
category_bar: true
---


# 机器学习与模式识别

## 前言

学科情况：

| 主讲教师 | 学分配额 | 学科类别 |
| :------: | :------: | :------: |
|  杨琬琪  |    3     |  专业课  |

成绩组成：

| 上机+课堂 | 课堂测验（3次） | 期末（闭卷） |
| :-------: | :-------------: | :----------: |
|    20%    |       30%       |     50%      |

教材情况：

|      课程名称      | 选用教材 | 版次 |  作者  |     出版社     |      ISBN号       |
| :----------------: | :------: | :--: | :----: | :------------: | :---------------: |
| 机器学习与模式识别 | 机器学习 |  --  | 周志华 | 清华大学出版社 | 978-7-302-42328-7 |

学习资源：

- :tv: 西瓜书视频、PPT 资源：[机器学习初步](https://www.xuetangx.com/learn/nju0802bt/nju0802bt/19322711/)、[PPT - 百度网盘链接](https://pan.baidu.com/s/1-OfnKcJ_bfWHcGHgzeQmpg?pwd=k3gk
    )
- :book: 西瓜书电子资源：[Machine_Learning/机器学习_周志华.pdf](https://github.com/jingyuexing/Ebook/blob/master/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)
- :tv: ​南瓜书视频资源：[《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集](https://www.bilibili.com/video/BV1Mh411e7VU/)
- :book: 南瓜书电子资源：[《机器学习》（西瓜书）公式详解](https://github.com/datawhalechina/pumpkin-book)

实验平台：

- AI Studio：https://aistudio.baidu.com/index

本地路径：

- :watermelon: 西瓜书电子书：[机器学习_周志华](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_周志华.pdf)
- :page_with_curl: 上课 PPT by 周志华：[机器学习\_课件\_周志华](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_课件_周志华)
- :page_with_curl: 上课 PPT by 杨琬琪：[机器学习\_课件\_杨琬琪](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_课件_杨琬琪)

## 第1章 绪论

### 1.1 引言

pass

### 1.2 基本术语

|          Name          |                         Introduction                         |
| :--------------------: | :----------------------------------------------------------: |
|      机器学习定义      | 利用**经验**改善系统自身性能，主要研究**智能数据分析**的理论和方法。 |
|      计算学习理论      | 最重要的理论模型是 PAC(Probably Approximately Correct, 概率近似正确) learning model，即以很高的概率得到很好的模型 $P(|f(x|- y \le \epsilon) \ge1 - \delta$ |
|         P 问题         |                 在多项式时间内计算出答案的解                 |
|        NP 问题         |                 在多项式时间内检验解的正确性                 |
|      特征（属性）      |                              --                              |
|     特征（属性）值     |                         连续 or 离散                         |
|        样本维度        |                       特征（属性）个数                       |
| 特征（属性、输入）空间 |                        特征张成的空间                        |
|    标记（输出）空间    |                        标记张成的空间                        |
|          样本          |                            \<x\>                             |
|          样例          |                           \<x,y\>                            |
|        预测任务        |  监督学习、无监督学习、半监督学习、噪音标记学习、多标记学习  |
|        泛化能力        |                 应对未来未见的测试样本的能力                 |
|     独立同分布假设     |                历史和未来的数据来自相同的分布                |
### 1.3 假设空间

假设空间：所有可能的样本组合构成的集合空间

版本空间：根据已知的训练集，将假设空间中与正例不同的、反例一致的样本全部删掉，剩下的样本组合构成的集合空间

### 1.4 归纳偏好

No Free Launch 理论，没有很好的算法，只有适合的算法。好的算法来自于对数据的好假设、好偏执，大胆假设，小心求证

### 1.5 发展历程

pass

### 1.6 应用现状

pass

## 第2章 模型评估与选择

### 2.1 经验误差与过拟合

概念辨析

- 错误率：针对测试数据而言，分错的样本数 $a$ 占总样本数 $m$ 的比例 $E=\frac{a}{m}$
- 经验误差：针对训练数据而言，随着训练轮数或模型的复杂度越高，经验误差越小

{% fold info @误差训练曲线 %}
![误差训练曲线](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403120921263.png)
{% endfold %}

过拟合解决方法

- Early Stopping (当发现有过拟合现象就停止训练)

- Penalizing Large Weight (在经验风险上加一个正则化项)

- Bagging 思想 (对同一样本用多个模型投票产生结果)

- **Boosting** 思想 (多个弱分类器增强分类能力，降低偏差)

- Dropconnection (神经网络全连接层中减少过拟合的发生) 

欠拟合解决方法

- 决策树：拓展分支

- 神经网络：增加训练轮数

### 2.2 评估方法

- **留出法（hold-out）：将数据集分为三个部分，分别为训练集、验证集、测试集**。测试集对于训练是完全未知的，我们划分出测试集是为了模拟未来未知的数据，因此当下的任务就是利用训练集和验证集训练出合理的模型来尽可能好的拟合测试集。那么如何使用划分出的训练集和验证集来训练、评估模型呢？就是根据模型的复杂度 or 模型训练的轮数，根据上图的曲线情况来选择模型。

- **交叉验证法（cross validation）：一般方法为 k 次 k 折交叉验证，即 k 次将数据随机划分为 k 个大小相似的互斥子集**。将其中 $k-1$ 份作为训练数据，$1$ 份作为测试数据，每轮执行 $k$ 次获得平均值。

- **自助法（bootstrapping）：有放回采样获得训练集**。每轮从数据集 $D$ 中（共 $m$ 个样本）有放回的采样 $m$ 次，这 $m$ 个抽出来的样本集合 $D'$ 大约占数据集的 $\frac{2}{3}$，于是就可以将抽出的样本集合 $D'$ 作为训练集，$D-D'$ 作为测试集即可

    {% fold info @测试集占比 1/3 证明过程 %}
    ![测试集占比 1/3 证明过程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403121131482.png)
    {% endfold %}

### 2.3 性能度量

#### 2.3.1 回归任务

- 均方误差：$\displaystyle MSE=\frac{1}{m} \sum_{i=1}^m(f(x_i) - y_i)^2$

- 均方根误差：$\displaystyle RMSE=\sqrt{\frac{1}{m} \sum_{i=1}^m(f(x_i) - y_i)^2}$

- $R^2$ 分数：$\displaystyle R^2 = 1 - \frac{\sum_{i=1}^m(f(x_i)-y_i)^2}{\sum_{i=1}^m(\bar{y} - y_i)^2},\quad \bar{y} = \frac{1}{m}\sum_{i=1}^m y_i$

    {% fold info @个人理解 %}
    首先理解各部分的含义。减数的分子表示预测数据的平方差，减数的分母表示真实数据的平方差。而平方差是用来描述数据离散程度的统计量。

    为了保证回归拟合的结果尽可能不受数据离散性的影响，我们通过相除来判断预测的数据是否离散。如果和原始数据离散性差不多，那么商就接近1，R方就接近0，表示性能较差，反之如果比原始数据离散性小，那么商就接近0，R方就接近1，表示性能较优。
    {% endfold %}

#### 2.3.2 分类任务

- 错误率：$\displaystyle E(f;D) = \frac{1}{m} \sum_{i=1}^mf(x_i \neq y_i)$

- 准确率：$\displaystyle A(f;D) = \frac{1}{m} \sum_{i=1}^mf(x_i = y_i)$

- 混淆矩阵

    - 查准率（precision）：$\displaystyle P = \frac{TP}{TP+FP}$ - 适用场景：商品搜索推荐（尽可能推荐出适当的商品即可，至于商品数量无所谓）
    - 查全率/召回率（recall）：$\displaystyle R = \frac{TP}{TP+FN}$ - 适用场景：逃犯、病例检测（尽可能将正例检测出来，至于查准率无所谓）
    - 准确率（accuracy）：$\displaystyle A = \frac{TP+TN}{TP+FN+FP+TN}$
    - F1 度量（F1-score）：$\displaystyle F_1 = \frac{2\times P \times R}{P + R}$​ - 用于综合查准率和查全率的指标
    
    {% fold info @分类结果混淆矩阵 %}
    ![分类结果混淆矩阵](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403121018205.png)
    {% endfold %}
    
    - 对于多分类问题，我们可以将该问题分解为多个二分类问题（ps：假设为 n 个）。从而可以获得多个上述的混淆矩阵，那么也就获得了多个 $P_i$、$R_i$ 以及全局均值 $\overline{TP}$、$\overline{FP}$、$\overline{FN}$，进而衍生出两个新的概念
    
        宏
    
        - 宏查准率：$\displaystyle macroP = \frac{1}{n} \sum_{i=1}^n P_i$
        - 宏查全率：$\displaystyle macroR = \frac{1}{n} \sum_{i=1}^n R_i$
        - 宏 $F1$：$\displaystyle macroF_1 = \frac{2 \times macroP \times macroR}{macroP+macroR}$
    
        微
    
        - 微查准率：$\displaystyle microP = \frac{\overline{TP}}{\overline{TP}+\overline{FP}}$
        - 微查全率：$\displaystyle microR = \frac{\overline{TP}}{\overline{TP}+\overline{FN}}$
        - 微 $F1$：$\displaystyle microF_1 = \frac{2 \times microP \times microR}{microP+microR}$
    
- P-R 曲线

    {% fold info @P-R 曲线趋势图 %}

    ![P-R 曲线趋势图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403190830311.png)

    {% endfold %}
    
    - 横纵坐标：横坐标为查全率（Recall），纵坐标为查准率（Precision）

    - 如何产生？我们根据学习器对于每一个样本的**预测值**（正例性的概率）进行降序排序，然后调整截断点将预测后的样本进行二分类，将截断点之前的所有数据全部认为**预测正例**，截断点之后的所有数据全部认为**预测反例**。然后计算两个指标进行绘图。

        {% fold light @什么分类任务中的是预测值？ %}
    我们知道学习器得到最终的结果一般不是一个绝对的二值，如 0,1。往往是一个连续的值，比如 [0,1]，也就是“正例性的概率”。因此我们才可以选择合适的截断点将所有的样本数据划分为两类。
        {% endfold %}
    
    - 趋势解读：随着截断点的值不断下降，很显然查全率 $R$ 会不断上升，查准率 $P$ 会不断下降
    
    - 不同曲线对应学习器的性能度量：**曲线与横纵坐标围成的面积**衡量了样本预测排序的质量。因此下图中 A 曲线的预测质量比 C 曲线的预测质量高。但是我们往往会遇到比较 A 与 B 的预测质量的情况，由于曲线与坐标轴围成的面积难以计算，因此我们引入了**平衡点**的概念。平衡点就是查准率与查询率相等的曲线，即 $P=R$ 的曲线。平衡点越往右上，学习器的预测性能越好。

- ROC 曲线与 AUC :star:

    {% fold info @ROC 曲线图 - 受试者工作特征 %}

    ![ROC 曲线图 - 受试者工作特征](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403190851371.png)
    
    {% endfold %}
    
    - 横纵坐标：横坐标为**假正例率** $\displaystyle FPR = \frac{FP}{FP+TN}$，纵坐标为**真正例率** $\displaystyle TPR = \frac{TP}{TP+FN}$
    
    - 如何产生？与 P-R 图的产生类似，只不过计算横纵坐标的规则不同，不再赘述。
    
    - 趋势解读：随着截断点的值不断下降，真正例率与假正例率均会不断上升，因为分子都是从 0 开始逐渐增加的
    
    - 不同曲线对应学习器的性能度量：**AUC** 衡量了样本预测的排序质量。AUC 即 ROC 曲线右下方的面积，面积越大则对应的预测质量更高，学习器性能更好。不同于上述引入平衡点的概念，此处的面积我们可以直接计算，甚至 1-AUC 也可以直接计算。
    
        我们定义 $\text{AUC}$ 的计算公式为：（其实就是每一块梯形的面积求和，ps：矩形也可以用梯形面积计算公式代替）
        $$
        \sum _{i=1}^{m-1} \frac{(y_{i}+y_{i+1}) \cdot (x_{i+1} - x_i)}{2}
        $$
        我们定义损失函数（$loss$） $l_{rank} = 1-AUC$ 的计算公式为：（ps：感觉下述公式不是很准，因为正反例预测值相等的比例比不一定就是一比一）
    
        ![损失函数计算公式](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403191055792.png)
        
    

### 2.4 比较检验

理论依据：统计假设检验（hypothesis test）

两个学习器性能比较：

- 交叉验证 t 检验：对于 k 折两个学习期产生的 k 个误差**之差**，求得其均值 $\mu$ 个方差 $\sigma ^2$，若变量 $\Gamma_t$ 小于临界值，则表明学习器没有显著差异，其中变量 $\Gamma_t$ 为
    $$
    \Gamma_t = |\frac{\sqrt{k}\mu}{\sigma}|
    $$

- McNemar 检验：对于二分类问题，我们可以得到下方的列联表

    {% fold info @二分类问题 - 列联表 %}

    ![列联表](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231541827.png)

    {% endfold %}

    若变量 $\Gamma_{\chi ^2}$ 小于临界值，则表明学习器没有显著差异，其中变量 $\Gamma_{\chi ^2}$ 为
    $$
    \Gamma_{\chi ^2} = \frac{(|e_{01} - e_{10}| - 1)^2}{e_{01}+e_{10}}
    $$

### 2.5 偏差与方差

现在我们得到了学习算法的泛化性能，我们还想知道为什么会有这样的泛化性能，即我们应该如何理论的解释这样的泛化性能呢？我们引入 **偏差-方差分解** 的概念来从理论的角度解释**期望泛化误差**。那么这个方法一定是完美解释的吗？也有一定的缺点，因此我们还会引入 **偏差-方差窘境** 的概念来解释**偏差和方差对于泛化误差的贡献**。

在此之前我们需要知道偏差、方差和噪声的基本定义：

- 偏差：学习算法的期望输出与真实结果的偏离程度，**刻画算法本身的拟合能力**。
- 方差：使用同规模的不同训练集进行训练时带来的性能变化，**刻画数据扰动带来的影响**。
- 噪声：当前任务上任何算法所能达到的期望泛化误差的**下界**（即不可能有算法取得更小的误差），**刻画问题本身的难度**。

#### 2.5.1 偏差-方差分解

我们定义以下符号：$x$ 为测试样本，$y_D$ 为 $x$ 在数据集中的标记，$y$ 为 $x$ 的真实标记，$f(x;D)$ 为模型在训练集 $D$ 上学习后的预测输出。

我们以回归任务为例：（下面的全部变量均为在所有相同规模的训练集下得到的**期望**结果）

- 输出：$\overline{f}(x) = E_D[f(x;D)]$
- 方差：$var(x) = E_D[(\overline{f}(x) - f(x;D))^2]$
- 偏差：$bias^2(x) = (\overline{f}(x) - y)^2$
- 噪声：$\epsilon ^2 = E_D[(y_D - y)^2]$

偏差-方差分解的结论：
$$
E(f;D) = bias^2(x) + var(x) + \epsilon^2
$$
{% fold info @偏差-方差分解结论推导 %}

![偏差-方差分解结论推导](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231651554.jpg)

{% endfold %}

解释说明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。因此给定一个学习任务，我们可以从偏差和方差两个角度入手进行优化，即需要使偏差较小（充分拟合数据），且需要使方差较小（使数据扰动产生的影响小）

#### 2.5.2 偏差-方差窘境

其实偏差和方差是有冲突的，这被称为偏差-方差窘境（bias-variance-dilemma）。对于以下的示意图我们可以知道：对于给定的学习任务。一开始拟合能力较差，学习器对于不同的训练数据不够敏感，此时泛化错误率主要来自偏差；随着训练的不断进行，学习器的拟合能力逐渐增强，对于数据的扰动更加敏感，使得方差主导了泛化错误率；在训练充分以后，数据的轻微扰动都可能导致预测输出发生显著的变化，此时方差就几乎完全主导了泛化错误率。

{% fold info @偏差-方差窘境 示意图 %}

![偏差-方差窘境 示意图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231628114.png)

{% endfold %}

## 第3章 线性模型

本章介绍机器学习模型之线性模型，将从学习任务展开。学习任务分别为：

- 回归（最小二乘法、岭回归）
- 二分类（对数几率（逻辑）回归、线性判别分析）
- 多分类（一对一、一对其余、多对多）

### 3.1 基本形式

$$
f(x_i) = w^T x_i + b \\
w = (w_1; w_2; \cdots; w_d), w \in R^d, b \in R
$$

线性模型的优点：形式简单、易于建模、高可解释性、非线性模型的基础

线性模型的缺点：线性不可分

### 3.2 线性回归

预测式已经在 3.1 中标明了，现在的问题就是，如何获得 $w$ 和 $b$ 使得预测值 $f(x)$ 与真实值 $y$ 尽可能的接近，也就是误差 $\epsilon = ||f(x) - y||$ 尽可能的小？在前面的 2.3 节性能度量中，我们知道对于一般的回归任务而言，可以通过均方误差来评判一个回归模型的性能。借鉴该思想，线性回归也采用**均方误差理论**，求解的目标函数就是使均方误差最小化。

在正式开始介绍求解参数 $w$ 和 $b$ 之前，我们先直观的理解一下均方误差。我们知道，均方误差对应了欧氏距离，即两点之间的欧几里得距离。于是在线性回归任务重，就是寻找一条直线使得所有的样本点距离该直线的距离之和尽可能的小。

基于均方误差最小化的模型求解方法被称为 **最小二乘法 (least squre method)**。而求解 $w$ 和 $b$ 使得目标函数 $E_{(w,b)} = \sum_{i=1}^{m}(y_i - f(x_i))$ 最小化的过程，被称为线性回归模型的 **最小二乘“参数估计” (parameter estimation)**。

{% note info %}

于是问题就转化为了**无约束的最优化问题求解**。接下来我们将从一元线性回归引入，进而推广到多维线性回归的参数求解，最后补充广义的线性回归与其他线性回归的例子。

{% endnote %}

#### 3.2.1 一元线性回归

现在假设只有一个属性 x，对应一维输出 y。现在我们试图根据已知的 \<x,y\> 样本数据学习出一个模型 $f(x_i) = wx_i+b$ 使得尽可能准确的预测未来的数据。那么此时如何求解模型中当目标函数取最小值时的参数 w 和 b 呢？很显然我们可以使用无约束优化问题的一阶必要条件求解。

{% fold info @一元线性回归：参数 w 和 b 的求解推导（式 3.7、式 3.8） %}

前置说明：在机器学习中，很少有闭式解（解析解），但是线性回归是特例，可以解出闭式解。

闭式解推导过程：

![一元线性回归：参数 w 和 b 的求解推导（式 3.7、式 3.8）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301501425.jpg)

{% endfold %}

#### 3.2.2 多元线性回归

现在我们保持输出不变，即 $y$ 仍然是一维，将输入的样本特征从一维扩展到 $d$ 维。现在同样适用最小二乘法，来计算 $w$ 和 $b$ 使得均方误差最小。只不过现在的 $w$ 是一个一维向量 $w = (w_1,w_2, \cdots , w_d)$

现在我们按照原来的方法进行求解。在求解之前我们采用向量的方式简化一下数据的表示，$X$ 为修改后的样本特征矩阵，$\hat w$ 为修改后的参数矩阵，$y$ 为样本标记值，$f(x)$ 为模型学习结果：
$$
X = 
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
\vdots & \vdots &  & \vdots & 1 \\
x_{m1} & x_{m2} & \cdots & x_{md} & 1
\end{bmatrix}

,

\hat w = (w;b) = 
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_d \\
b
\end{bmatrix}

,

y = 
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{bmatrix}

,

f(x) = 
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_m)
\end{bmatrix}
 = 
\begin{bmatrix}
x_1 ^ T \hat w \\
x_2 ^ T \hat w \\
\vdots \\
x_d ^ T \hat w 
\end{bmatrix}
$$
于是损失函数 $E_{\hat w}$ 就定义为：
$$
E_{\hat w} = (y - X \hat w) ^T (y - X \hat w)
$$
我们用同样的方法求解其闭式解：

{% fold info @多元线性回归：参数 w 的求解推导（式 3.10） %}

![多元线性回归：参数 w 的求解推导（式 3.10）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301607772.jpg)

我们不能直接等式两边同 $\times$ 矩阵 $X^TX$ 的逆，因为**不清楚其是否可逆**，于是进行下面的两种分类讨论：

1. $X^T X$​ 可逆：则参数 $\hat w ^* = (X^TX)^{-1}X^Ty$，令样本 $\hat x_i = (x_i,1)$，则线性回归模型为：
    $$
    f(x_i) = \hat x_i ^T \hat w^*
    $$

2. $X^T X$ 不可逆：我们引入 $L_2$ 正则化项 $\alpha || \hat w ||^2$

    现在的损失函数就定义为：
    $$
    E_{\hat w} = (y - X \hat w) ^T (y - X \hat w) + \alpha || \hat w ||^2
    $$
    同样将损失函数对参数向量 $\hat w$ 求偏导，得：
    $$
    \begin{aligned}
    \frac{\partial E_{\hat w}}{\partial \hat w} &= \cdots \\
    &= 2X^TX\hat w - 2 X^T y + 2 \alpha \hat w \\
    &= 2 X ^T(X \hat w - y) + 2 \alpha \hat w
    \end{aligned}
    $$
    我们令其为零，得参数向量 $\hat w$ 为：
    $$
    \hat w = (X^T X + \alpha T)^{-1} X^T y
    $$

{% endfold %}

#### 3.2.3 广义线性回归

可否令模型的预测值 $w^Tx+b$ 逼近 $y$ 的衍生物？我们以**对数线性回归**为例，令：
$$
\ln y = w^Tx+b
$$
本质上我们训练的线性模型 $\ln y = w^Tx+b$ 现在可以拟合非线性数据 $y = e^{w^Tx+b}$。更广义的来说，就是让训练的线性模型去拟合：
$$
y = g^{-1}(w^Tx+b)
$$
此时得到的线性模型 $g(y) = w^Tx +b$ 被称为**广义线性模型**，要求非线性函数 $g(\cdot)$ 是单调可微的。而此处的对数线性回归其实就是广义线性模型在 $g(\cdot) = \ln (\cdot)$ 时的特例

{% fold info @线性模型拟合非线性数据图例：对数线性回归 %}

![线性模型拟合非线性数据](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301634748.png)

{% endfold %}

#### 3.2.4 其他线性回归

- 支持向量机回归

- 决策树回归

- 随机森林回归

- [LASSO 回归](https://scikit-learn.org.cn/view/411.html)：增加 $L_1$ 正则化项

    ![LASSO 回归](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403260936320.png)

- [ElasticNet 回归](https://scikit-learn.org.cn/view/404.html)：增加 $L_1$ 和 $L_2$ 正则化项

    ![ElasticNet 回归](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403260936147.png)

- XGBoost 回归

### 3.3 对数几率回归

对于**二分类**任务。

单位阶跃函数。

对数几率函数。俗称逻辑回归（logistic function），旨在线性回归模型的基础上，使用 $Logistic$ 函数，将线性模型 $w^Tx+b$ 的结果压缩到 $[0,1]$​ 之间，使其拥有概率意义。其本质仍是一个线性的**分类**模型。

为了求解其中的 $w$ 和 $b$​ 参数，我们采用**极大似然估计法**。即判断每一组 $w$ 和 $b$ 的使得每一个样本被划分到正确的类别的几率有多高。

### 3.4 线性判别分析

pass

### 3.5 多分类学习



### 3.6 类别不平衡问题



## 第4章 决策树



## 第5章 神经网络



## 第6章 支持向量机



## 第7章 贝叶斯分类



## 第8章 集成学习



## 第9章 聚类



## 第10章 降维与度量学习 *



## 第11章 特征选择与稀疏学习 *



## ~~第12章 计算学习理论~~



## 第13章 半监督学习



## 第14章 概率图模型



## ~~第15章 规则学习~~



## 第16章 强化学习

