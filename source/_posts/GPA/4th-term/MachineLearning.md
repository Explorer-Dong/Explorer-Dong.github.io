---
title: MachineLearning
categories:
  - GPA
  - 4th-term
category_bar: true
---


## 《机器学习与模式识别》

## 前言

学科地位：

| 主讲教师 | 学分配额 | 学科类别 |
| :------: | :------: | :------: |
|  杨琬琪  |    3     |  专业课  |

成绩组成：

| 上机+课堂 | 课堂测验（3次） | 期末（闭卷） |
| :-------: | :-------------: | :----------: |
|    20%    |       30%       |     50%      |

教材情况：

|      课程名称      | 选用教材 | 版次 |  作者  |     出版社     |      ISBN号       |
| :----------------: | :------: | :--: | :----: | :------------: | :---------------: |
| 机器学习与模式识别 | 机器学习 |  --  | 周志华 | 清华大学出版社 | 978-7-302-42328-7 |

学习资源：

- :watermelon: 西瓜书 :tv: 视频、PPT 资源：[机器学习初步](https://www.xuetangx.com/learn/nju0802bt/nju0802bt/19322711/)、[PPT - 百度网盘链接](https://pan.baidu.com/s/1-OfnKcJ_bfWHcGHgzeQmpg?pwd=k3gk
    )
- :watermelon: 西瓜书 :book: 电子资源：[Machine_Learning/机器学习_周志华.pdf](https://github.com/jingyuexing/Ebook/blob/master/Machine_Learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%91%A8%E5%BF%97%E5%8D%8E.pdf)
- 🎃 南瓜书 :tv: 视频资源：《机器学习公式详解》（南瓜书）与西瓜书公式推导直播合集
- 🎃 南瓜书 :book: 电子资源：[《机器学习》（西瓜书）公式详解](https://github.com/datawhalechina/pumpkin-book)

实验平台：

- AI Studio：https://aistudio.baidu.com/index

本地路径：

- :watermelon: 西瓜书电子书：[机器学习_周志华](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_周志华.pdf)
- 🎃 南瓜书电子书：[pumpkin_book](D:\华为云盘\2. Score\4. 机器学习与模式识别\pumpkin-book\pdf\pumpkin_book_1.9.9.pdf)
- :page_with_curl: 上课 PPT by 周志华：[机器学习\_课件\_周志华](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_课件_周志华)
- :page_with_curl: 上课 PPT by 杨琬琪：[机器学习\_课件\_杨琬琪](D:\华为云盘\2. Score\4. 机器学习与模式识别\机器学习_课件_杨琬琪)

## 第1章 绪论

### 1.1 引言

pass

### 1.2 基本术语

|          Name          |                         Introduction                         |
| :--------------------: | :----------------------------------------------------------: |
|      机器学习定义      | 利用**经验**改善系统自身性能，主要研究**智能数据分析**的理论和方法。 |
|      计算学习理论      | 最重要的理论模型是 PAC(Probably Approximately Correct, 概率近似正确) learning model，即以很高的概率得到很好的模型 $P(|f(x|- y \le \epsilon) \ge1 - \delta$ |
|         P 问题         |                 在多项式时间内计算出答案的解                 |
|        NP 问题         |                 在多项式时间内检验解的正确性                 |
|      特征（属性）      |                              --                              |
|     特征（属性）值     |                         连续 or 离散                         |
|        样本维度        |                       特征（属性）个数                       |
| 特征（属性、输入）空间 |                        特征张成的空间                        |
|    标记（输出）空间    |                        标记张成的空间                        |
|          样本          |                            \<x\>                             |
|          样例          |                           \<x,y\>                            |
|        预测任务        |  监督学习、无监督学习、半监督学习、噪音标记学习、多标记学习  |
|        泛化能力        |                 应对未来未见的测试样本的能力                 |
|     独立同分布假设     |                历史和未来的数据来自相同的分布                |
### 1.3 假设空间

假设空间：所有可能的样本组合构成的集合空间

版本空间：根据已知的训练集，将假设空间中与正例不同的、反例一致的样本全部删掉，剩下的样本组合构成的集合空间

### 1.4 归纳偏好

No Free Launch 理论，没有很好的算法，只有适合的算法。好的算法来自于对数据的好假设、好偏执，大胆假设，小心求证

### 1.5 发展历程

pass

### 1.6 应用现状

pass

## 第2章 模型评估与选择

### 2.1 经验误差与过拟合

概念辨析

- 错误率：针对测试数据而言，分错的样本数 $a$ 占总样本数 $m$ 的比例 $E=\frac{a}{m}$
- 经验误差：针对训练数据而言，随着训练轮数或模型的复杂度越高，经验误差越小

{% fold info @误差训练曲线 %}
![误差训练曲线](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403120921263.png)
{% endfold %}

过拟合解决方法

- Early Stopping (当发现有过拟合现象就停止训练)

- Penalizing Large Weight (在经验风险上加一个正则化项)

- Bagging 思想 (对同一样本用多个模型投票产生结果)

- **Boosting** 思想 (多个弱分类器增强分类能力，降低偏差)

- Dropconnection (神经网络全连接层中减少过拟合的发生) 

欠拟合解决方法

- 决策树：拓展分支

- 神经网络：增加训练轮数

### 2.2 评估方法

- **留出法（hold-out）：将数据集分为三个部分，分别为训练集、验证集、测试集**。测试集对于训练是完全未知的，我们划分出测试集是为了模拟未来未知的数据，因此当下的任务就是利用训练集和验证集训练出合理的模型来尽可能好的拟合测试集。那么如何使用划分出的训练集和验证集来训练、评估模型呢？就是根据模型的复杂度 or 模型训练的轮数，根据上图的曲线情况来选择模型。

- **交叉验证法（cross validation）：一般方法为 k 次 k 折交叉验证，即 k 次将数据随机划分为 k 个大小相似的互斥子集**。将其中 $k-1$ 份作为训练数据，$1$ 份作为测试数据，每轮执行 $k$ 次获得平均值。

- **自助法（bootstrapping）：有放回采样获得训练集**。每轮从数据集 $D$ 中（共 $m$ 个样本）有放回的采样 $m$ 次，这 $m$ 个抽出来的样本集合 $D'$ 大约占数据集的 $\frac{2}{3}$，于是就可以将抽出的样本集合 $D'$ 作为训练集，$D-D'$ 作为测试集即可

    {% fold info @测试集占比 1/3 证明过程 %}
    ![测试集占比 1/3 证明过程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403121131482.png)
    {% endfold %}

### 2.3 性能度量

#### 2.3.1 回归任务

- 均方误差：$\displaystyle MSE=\frac{1}{m} \sum_{i=1}^m(f(x_i) - y_i)^2$

- 均方根误差：$\displaystyle RMSE=\sqrt{\frac{1}{m} \sum_{i=1}^m(f(x_i) - y_i)^2}$

- $R^2$ 分数：$\displaystyle R^2 = 1 - \frac{\sum_{i=1}^m(f(x_i)-y_i)^2}{\sum_{i=1}^m(\bar{y} - y_i)^2},\quad \bar{y} = \frac{1}{m}\sum_{i=1}^m y_i$

    {% fold info @个人理解 %}
    首先理解各部分的含义。减数的分子表示预测数据的平方差，减数的分母表示真实数据的平方差。而平方差是用来描述数据离散程度的统计量。

    为了保证回归拟合的结果尽可能不受数据离散性的影响，我们通过相除来判断预测的数据是否离散。如果和原始数据离散性差不多，那么商就接近1，R方就接近0，表示性能较差，反之如果比原始数据离散性小，那么商就接近0，R方就接近1，表示性能较优。
    {% endfold %}

#### 2.3.2 分类任务

- 错误率（error）：$\displaystyle E(f;D) = \frac{1}{m} \sum_{i=1}^mf(x_i \neq y_i)$

- 准确率（accuracy）：$\displaystyle A(f;D) = \frac{1}{m} \sum_{i=1}^mf(x_i = y_i)$

- 混淆矩阵

    {% fold info @图例 %}

    ![混淆矩阵 - 图例](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403121018205.png)

    {% endfold %}

    - **查准率/精度（precision）**：$\displaystyle P = \frac{TP}{TP+FP}$ - 适用场景：商品搜索推荐（尽可能推荐出适当的商品即可，至于商品数量无所谓）
    - **查全率/召回率（recall）**：$\displaystyle R = \frac{TP}{TP+FN}$ - 适用场景：逃犯、病例检测（尽可能将正例检测出来，至于查准率无所谓）
    - **F1 度量（F1-score）**：$\displaystyle F_1 = \frac{2\times P \times R}{P + R}$​ - 用于综合查准率和查全率的指标
    
    - 对于**多分类问题**，我们可以将该问题分解为多个二分类问题（ps：假设为 n 个）。从而可以获得多个上述的混淆矩阵，那么也就获得了多个 $P_i$、$R_i$ 以及全局均值 $\overline{TP}$、$\overline{FP}$、$\overline{FN}$，进而衍生出两个新的概念
    
        - **宏**
          - 宏查准率：$\displaystyle macroP = \frac{1}{n} \sum_{i=1}^n P_i$
          - 宏查全率：$\displaystyle macroR = \frac{1}{n} \sum_{i=1}^n R_i$
          - 宏 $F1$：$\displaystyle macroF_1 = \frac{2 \times macroP \times macroR}{macroP+macroR}$
          
        - **微**
        
          - 微查准率：$\displaystyle microP = \frac{\overline{TP}}{\overline{TP}+\overline{FP}}$
          - 微查全率：$\displaystyle microR = \frac{\overline{TP}}{\overline{TP}+\overline{FN}}$
          - 微 $F1$：$\displaystyle microF_1 = \frac{2 \times microP \times microR}{microP+microR}$
    
- P-R 曲线

    {% fold info @图例 %}

    ![P-R 曲线趋势图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403190830311.png)

    {% endfold %}
    
    - 横纵坐标：横坐标为查全率（Recall），纵坐标为查准率（Precision）

    - 如何产生？我们根据学习器对于每一个样本的**预测值**（正例性的概率）进行降序排序，然后调整截断点将预测后的样本进行二分类，将截断点之前的所有数据全部认为**预测正例**，截断点之后的所有数据全部认为**预测反例**。然后计算两个指标进行绘图。

        {% fold light @什么分类任务中的是预测值？ %}
    我们知道学习器得到最终的结果一般不是一个绝对的二值，如 0,1。往往是一个连续的值，比如 [0,1]，也就是“正例性的概率”。因此我们才可以选择合适的截断点将所有的样本数据划分为两类。
        {% endfold %}
    
    - 趋势解读：随着截断点的值不断下降，很显然查全率 $R$ 会不断上升，查准率 $P$ 会不断下降
    
    - 不同曲线对应学习器的性能度量：**曲线与横纵坐标围成的面积**衡量了样本预测排序的质量。因此下图中 A 曲线的预测质量比 C 曲线的预测质量高。但是我们往往会遇到比较 A 与 B 的预测质量的情况，由于曲线与坐标轴围成的面积难以计算，因此我们引入了**平衡点**的概念。平衡点就是查准率与查询率相等的曲线，即 $P=R$ 的曲线。平衡点越往右上，学习器的预测性能越好。

- ROC 曲线与 AUC :star:

    {% fold info @图例 %}

    ![ROC 曲线图 - 受试者工作特征](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403190851371.png)
    
    {% endfold %}
    
    - 横纵坐标：横坐标为**假正例率** $\displaystyle FPR = \frac{FP}{FP+TN}$，纵坐标为**真正例率** $\displaystyle TPR = \frac{TP}{TP+FN}$
    
    - 如何产生？与 P-R 图的产生类似，只不过计算横纵坐标的规则不同，不再赘述。
    
    - 趋势解读：随着截断点的值不断下降，真正例率与假正例率均会不断上升，因为分子都是从 0 开始逐渐增加的
    
    - 不同曲线对应学习器的性能度量：**AUC** 衡量了样本预测的排序质量。AUC 即 ROC 曲线右下方的面积，面积越大则对应的预测质量更高，学习器性能更好。不同于上述引入平衡点的概念，此处的面积我们可以直接计算，甚至 1-AUC 也可以直接计算。
    
        我们定义 $\text{AUC}$ 的计算公式为：（其实就是每一块梯形的面积求和，ps：矩形也可以用梯形面积计算公式代替）
        $$
        \sum _{i=1}^{m-1} \frac{(y_{i}+y_{i+1}) \cdot (x_{i+1} - x_i)}{2}
        $$
        我们定义损失函数（$loss$） $l_{rank} = 1-AUC$ 的计算公式为：（ps：感觉下述公式不是很准，因为正反例预测值相等的比例比不一定就是一比一）
    
        ![损失函数计算公式](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403191055792.png)
        
    

### 2.4 比较检验

理论依据：统计假设检验（hypothesis test）

两个学习器性能比较：

- 交叉验证 t 检验：对于 k 折两个学习期产生的 k 个误差**之差**，求得其均值 $\mu$ 个方差 $\sigma ^2$，若变量 $\Gamma_t$ 小于临界值，则表明学习器没有显著差异，其中变量 $\Gamma_t$ 为
    $$
    \Gamma_t = |\frac{\sqrt{k}\mu}{\sigma}|
    $$

- McNemar 检验：对于二分类问题，我们可以得到下方的列联表

    ![列联表](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231541827.png)

    若变量 $\Gamma_{\chi ^2}$ 小于临界值，则表明学习器没有显著差异，其中变量 $\Gamma_{\chi ^2}$ 为
    $$
    \Gamma_{\chi ^2} = \frac{(|e_{01} - e_{10}| - 1)^2}{e_{01}+e_{10}}
    $$

### 2.5 偏差与方差

现在我们得到了学习算法的泛化性能，我们还想知道为什么会有这样的泛化性能，即我们应该如何理论的解释这样的泛化性能呢？我们引入 **偏差-方差分解** 的概念来从理论的角度解释**期望泛化误差**。那么这个方法一定是完美解释的吗？也有一定的缺点，因此我们还会引入 **偏差-方差窘境** 的概念来解释**偏差和方差对于泛化误差的贡献**。

在此之前我们需要知道偏差、方差和噪声的基本定义：

- 偏差：学习算法的期望输出与真实结果的偏离程度，**刻画算法本身的拟合能力**。
- 方差：使用同规模的不同训练集进行训练时带来的性能变化，**刻画数据扰动带来的影响**。
- 噪声：当前任务上任何算法所能达到的期望泛化误差的**下界**（即不可能有算法取得更小的误差），**刻画问题本身的难度**。

#### 2.5.1 偏差-方差分解

我们定义以下符号：$x$ 为测试样本，$y_D$ 为 $x$ 在数据集中的标记，$y$ 为 $x$ 的真实标记，$f(x;D)$ 为模型在训练集 $D$ 上学习后的预测输出。

我们以回归任务为例：（下面的全部变量均为在所有相同规模的训练集下得到的**期望**结果）

- 输出：$\overline{f}(x) = E_D[f(x;D)]$
- 方差：$var(x) = E_D[(\overline{f}(x) - f(x;D))^2]$
- 偏差：$bias^2(x) = (\overline{f}(x) - y)^2$
- 噪声：$\epsilon ^2 = E_D[(y_D - y)^2]$

偏差-方差分解的结论：
$$
E(f;D) = bias^2(x) + var(x) + \epsilon^2
$$
{% fold info @推导 %}

![偏差-方差分解结论推导](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231651554.jpg)

{% endfold %}

解释说明：泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。因此给定一个学习任务，我们可以从偏差和方差两个角度入手进行优化，即需要使偏差较小（充分拟合数据），且需要使方差较小（使数据扰动产生的影响小）

#### 2.5.2 偏差-方差窘境

其实偏差和方差是有冲突的，这被称为偏差-方差窘境（bias-variance-dilemma）。对于以下的示意图我们可以知道：对于给定的学习任务。一开始拟合能力较差，学习器对于不同的训练数据不够敏感，此时泛化错误率主要来自偏差；随着训练的不断进行，学习器的拟合能力逐渐增强，对于数据的扰动更加敏感，使得方差主导了泛化错误率；在训练充分以后，数据的轻微扰动都可能导致预测输出发生显著的变化，此时方差就几乎完全主导了泛化错误率。

{% fold info @图例 %}

![偏差-方差窘境 示意图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403231628114.png)

{% endfold %}

## 第3章 线性模型

本章介绍机器学习模型之线性模型，将从学习任务展开。学习任务分别为：

- 回归（最小二乘法、岭回归）
- 二分类（对数几率（逻辑）回归、线性判别分析）
- 多分类（一对一、一对其余、多对多）

### 3.1 基本形式

$$
\begin{aligned}
f(x_i) &= w^T x_i + b \\
w &= (w_1; w_2; \cdots; w_d), w \in R^d, b \in R
\end{aligned}
$$

线性模型的优点：形式简单、易于建模、高可解释性、非线性模型的基础

线性模型的缺点：线性不可分

### 3.2 线性回归

{% note info %}

预测式已经在 3.1 中标明了，现在的问题就是，如何获得 $w$ 和 $b$ 使得预测值 $f(x)$ 与真实值 $y$ 尽可能的接近，也就是误差 $\epsilon = ||f(x) - y||$ 尽可能的小？在前面的 2.3 节性能度量中，我们知道对于一般的回归任务而言，可以通过均方误差来评判一个回归模型的性能。借鉴该思想，线性回归也采用**均方误差理论**，求解的目标函数就是使均方误差最小化。

在正式开始介绍求解参数 $w$ 和 $b$ 之前，我们先直观的理解一下均方误差。我们知道，均方误差对应了欧氏距离，即两点之间的欧几里得距离。于是在线性回归任务重，就是寻找一条直线使得所有的样本点距离该直线的距离之和尽可能的小。

基于均方误差最小化的模型求解方法被称为 **最小二乘法 (least squre method)**。而求解 $w$ 和 $b$ 使得目标函数 $E_{(w,b)} = \sum_{i=1}^{m}(y_i - f(x_i))$ 最小化的过程，被称为线性回归模型的 **最小二乘“参数估计” (parameter estimation)**。

于是问题就转化为了**无约束的最优化问题求解**。接下来我们将从一元线性回归引入，进而推广到多维线性回归的参数求解，最后补充广义的线性回归与其他线性回归的例子。

{% endnote %}

#### 3.2.1 一元线性回归

现在假设只有一个属性 x，对应一维输出 y。现在我们试图根据已知的 \<x,y\> 样本数据学习出一个模型 $f(x_i) = wx_i+b$ 使得尽可能准确的预测未来的数据。那么此时如何求解模型中当目标函数取最小值时的参数 w 和 b 呢？很显然我们可以使用无约束优化问题的一阶必要条件求解。

{% fold info @参数 w 和 b 的求解推导（式 3.7、式 3.8） %}

前置说明：在机器学习中，很少有闭式解（解析解），但是线性回归是特例，可以解出闭式解。

闭式解推导过程：

![一元线性回归：参数 w 和 b 的求解推导（式 3.7、式 3.8）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301501425.jpg)

{% endfold %}

#### 3.2.2 多元线性回归

现在我们保持输出不变，即 $y$ 仍然是一维，将输入的样本特征从一维扩展到 $d$ 维。现在同样适用最小二乘法，来计算 $w$ 和 $b$ 使得均方误差最小。只不过现在的 $w$ 是一个一维向量 $w = (w_1,w_2, \cdots , w_d)$

现在我们按照原来的方法进行求解。在求解之前我们采用向量的方式简化一下数据的表示，$X$ 为修改后的样本特征矩阵，$\hat w$ 为修改后的参数矩阵，$y$ 为样本标记值，$f(x)$ 为模型学习结果：
$$
X = 
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
\vdots & \vdots &  & \vdots & 1 \\
x_{m1} & x_{m2} & \cdots & x_{md} & 1
\end{bmatrix}

,

\hat w = (w;b) = 
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_d \\
b
\end{bmatrix}

,

y = 
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_m
\end{bmatrix}

,

f(x) = 
\begin{bmatrix}
f(x_1) \\
f(x_2) \\
\vdots \\
f(x_m)
\end{bmatrix}
 = 
\begin{bmatrix}
x_1 ^ T \hat w \\
x_2 ^ T \hat w \\
\vdots \\
x_d ^ T \hat w 
\end{bmatrix}
$$
于是损失函数 $E_{\hat w}$ 就定义为：
$$
E_{\hat w} = (y - X \hat w) ^T (y - X \hat w)
$$
我们用同样的方法求解其闭式解：

{% fold info @参数 w 的求解推导（式 3.10） %}

![多元线性回归：参数 w 的求解推导（式 3.10）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301607772.jpg)

我们不能直接等式两边同 $\times$ 矩阵 $X^TX$ 的逆，因为**不清楚其是否可逆**，于是进行下面的两种分类讨论：

1. $X^T X$​ 可逆：则参数 $\hat w ^* = (X^TX)^{-1}X^Ty$，令样本 $\hat x_i = (x_i,1)$，则线性回归模型为：
    $$
    f(x_i) = \hat x_i ^T \hat w^*
    $$

2. $X^T X$ 不可逆：我们引入 $L_2$ 正则化项 $\alpha || \hat w ||^2$

    现在的损失函数就定义为：
    $$
    E_{\hat w} = (y - X \hat w) ^T (y - X \hat w) + \alpha || \hat w ||^2
    $$
    同样将损失函数对参数向量 $\hat w$ 求偏导，得：
    $$
    \begin{aligned}
    \frac{\partial E_{\hat w}}{\partial \hat w} &= \cdots \\
    &= 2X^TX\hat w - 2 X^T y + 2 \alpha \hat w \\
    &= 2 X ^T(X \hat w - y) + 2 \alpha \hat w
    \end{aligned}
    $$
    我们令其为零，得参数向量 $\hat w$ 为：
    $$
    \hat w = (X^T X + \alpha I)^{-1} X^T y
    $$

{% endfold %}

#### 3.2.3 广义线性回归

可否令模型的预测值 $w^Tx+b$ 逼近 $y$ 的衍生物？我们以**对数线性回归**为例，令：
$$
\ln y = w^Tx+b
$$
本质上我们训练的线性模型 $\ln y = w^Tx+b$ 现在可以拟合非线性数据 $y = e^{w^Tx+b}$。更广义的来说，就是让训练的线性模型去拟合：
$$
y = g^{-1}(w^Tx+b)
$$
此时得到的线性模型 $g(y) = w^Tx +b$ 被称为**广义线性模型**，要求非线性函数 $g(\cdot)$ 是单调可微的。而此处的对数线性回归其实就是广义线性模型在 $g(\cdot) = \ln (\cdot)$ 时的特例

{% fold info @线性模型拟合非线性数据图例：对数线性回归 %}

![线性模型拟合非线性数据](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403301634748.png)

{% endfold %}

#### 3.2.4 其他线性回归

- 支持向量机回归

- 决策树回归

- 随机森林回归

- [LASSO 回归](https://scikit-learn.org.cn/view/411.html)：增加 $L_1$ 正则化项

    ![LASSO 回归](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403260936320.png)

- [ElasticNet 回归](https://scikit-learn.org.cn/view/404.html)：增加 $L_1$ 和 $L_2$ 正则化项

    ![ElasticNet 回归](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202403260936147.png)

- XGBoost 回归

### 3.3 对数几率回归

{% note info %}

对数几率回归准确来说应该叫逻辑回归，并且**不是回归**的学习任务，**而是二分类**的学习任务。本目将从**阈值函数的选择**和**模型参数的求解**两个维度展开讲解。

{% endnote %}

#### 3.3.1 阈值函数的选择

对于**二分类**任务。我们可以将线性模型的输出结果 $z=w^Tx+b$ 通过阈值函数 $g(\cdot)$ 进行映射，然后根据映射结果 $y=g(z)$ 进行二分类。那么什么样的非线性函数可以胜任阈值函数一职呢？

1.  最简单的一种阈值函数就是**单位阶跃函数**。映射关系如下。但是有一个问题就是单位阶跃函数不是单调可微函数，因此不可取

    {% fold light @为什么阈值函数需要单调可微呢？ %}

    一直有一个疑问，单位跃阶函数已经可以将线性模型的进行二值映射了，干嘛还要求阈值函数的反函数呢？

    其实是因为知道的太片面了。我们的终极目的是为了通过训练数据，学习出线性模型中的 $w$ 和 $b$ 参数。在《最优化方法》课程中我们知道，优化问题是需要进行迭代的，在迭代寻找最优解（此处就是寻找最优参数）的过程中，我们需要不断的检验当前解，以及计算修正量。仅仅知道线性模型 $z=w^Tx+b$ 关于阈值函数 $g(\cdot)$ 的映射结果 $y=g(z)$ 是完全不够的，因为我们在检验解、计算修正量时，采用梯度下降等算法来计算损失函数关于参数 $w$ 和 $b$ 的导数时，需要进行求导操作，比如上述损失函数 $E_{(w,b)} = \sum_{i=1}^m (y_i - g(x_i))^2$，如果其中的 $g(\cdot)$ 不可导，自然也就没法继续计算损失函数关于参数的导数了。

    至此疑问解决！让我们畅快的学习单调可微的阈值函数吧！

    {% endfold %}
    $$
    y = 
    \begin{cases}
    0, &z < 0 \\
    0.5,& z=0 \\
    1, & z>0
    \end{cases}
    $$

2.  常用的一种阈值函数是**对数几率函数**，也叫**逻辑函数（$\text{logistic function}$）**。映射关系如下：
    $$
    y = \frac{1}{1+e^{-z}}
    $$

#### 3.3.2 模型参数的求解

现在我们站在前人的肩膀上学习到了一种阈值函数：逻辑函数（$\text{logistic function}$）。在开始讲解参数 $w$ 和 $b$ 的求解过程之前，我们先解决一个疑问：为什么英文名是 $\text{logistic}$​，中文翻译却成了 **对数几率** 函数呢？

{% fold light @“对数几率”名称解读 %}

这就要从逻辑函数的实际意义出发了。对于逻辑函数，我们代入线性模型，并做以下转化：
$$
\begin{aligned}
& y = \frac{1}{1 + e^{-z}} \\
& y = \frac{1}{1 + e^{-(w^Tx+b)}} \\
& \ln \frac{y}{1 - y} = w^Tx+b \\
\end{aligned}
$$
若我们定义 $y$ 为样本 $x$ 是正例的可能性，则 $1-y$ 显然就是反例的可能性。两者比值 $\frac{y}{1-y}$ 就是几率，取对数 $\ln{\frac{y}{1-y}}$ 就是对数几率。

{% endfold %}

知道了逻辑函数的实际意义是**真实标记的对数几率**以后，接下来我们实际意义出发，讲解模型参数 $w$ 和 $b$​ 的推导过程。
{% fold info @推导 %}

若我们将 $y$ 视作类后验概率估计 $p(y=1 \ | \ x)$​，则有
$$
\ln \frac{p(y=1 \ | \ x)}{p(y=0 \ | \ x)} = w^Tx+b
$$
同时，显然有
$$
\begin{aligned}
p(y=1 \ | \ x) = \frac{1}{1 + e^{-(w^Tx+b)}} = \frac{e^{w^Tx+b}}{1 + e^{w^Tx+b}} \\
p(y=0 \ | \ x) = \frac{e^{-(w^Tx+b)}}{1 + e^{-(w^Tx+b)}} = \frac{1}{1 + e^{w^Tx+b}}
\end{aligned}
$$
于是我们可以**确定目标函数**了。与上述线性回归参数求解时，使均方误差最小从而求得参数类似，我们取当前目标函数为**对数似然函数**：
$$
\arg \max_{w,b} l(w,b) = \sum_{i=1}^m \ln p(y_i\ | \ x_i;w,b)
$$
当上述目标函数取最大值时，得到的参数 $w$ 和 $b$​ 即为所求。目标函数尽可能大就是样本属于真实标记的概率尽可能大。

我们将变量进行一定的变形：
$$
\begin{aligned}
\begin{cases}
\beta = (w;b) \\
\hat x = (x;1) \\
\end{cases}

&\to w^Tx + b = \beta^T\hat x \\

\begin{cases}
p_1(\hat x; \beta) = p(y = 1 \ | \ \hat x; \beta) \\
p_0(\hat x; \beta) = p(y = 0 \ | \ \hat x; \beta) \\
\end{cases}

&\to p(y_i\ | \ x_i;w,b) = y_i p_1(\hat x; \beta) + (1 - y_i) p_0(\hat x; \beta)

\end{aligned}
$$
于是上述对数似然函数就可以进行以下转化：
$$
\begin{aligned}

l(w,b) &= l(\beta) \\
&= \sum_{i=1}^m \ln \left [y_i p_1(\hat x; \beta) + (1 - y_i) p_0(\hat x; \beta) \right ] \\
&= \sum_{i=1}^m \ln \left [y_i p(y = 1 \ | \ \hat x; \beta) + (1 - y_i) p(y = 0 \ | \ \hat x; \beta) \right ] \\
&= \sum_{i=1}^m \ln \left [ y_i \frac{e^{\beta^T\hat x}}{1 + e^{\beta^T\hat x}} + (1-y_i) \frac{1}{1 + e^{\beta^T\hat x}} \right ] \\
&= \sum_{i=1}^m \ln \left [ \frac{y_i \beta^T\hat x +1 -y_i}{1 + e^{\beta^T\hat x}} \right ] \\
&= 
\begin{cases}
\sum_{i=1}^m \ln \left ( \frac{1}{1 + e^{\beta^T\hat x}} \right ), & y_i=0  \\
\sum_{i=1}^m \ln \left ( \frac{e^{\beta^T\hat x}}{1 + e^{\beta^T\hat x}} \right ), & y_i=1\\
\end{cases}\\
&= \sum_{i=1}^m \ln \left ( \frac{y_i e^{\beta^T\hat x}}{1 + e^{\beta^T\hat x}} \right ) \\
&= \sum_{i=1}^m \left( y_i e^{\beta^T\hat x} - \ln({1 + e^{\beta^T\hat x}})\right )
\end{aligned}
$$
进而将上述求解对数似然函数的最大值，转化为了求解上述转化后的目标函数的最小值时，对应的参数 $\beta$ 的值：
$$
\arg \min_{\beta} l(\beta) = \sum_{i=1}^m \left(- y_i e^{\beta^T\hat x} + \ln({1 + e^{\beta^T\hat x}})\right )
$$
由于上式是关于 $\beta$ 的高阶可导连续凸函数，因此我们有很多数值优化算法可以求得最优解时的参数值，比如梯度下降法、拟牛顿法等等。我们以牛顿法（Newton Method）为例：

目标：
$$
\beta ^* = \arg \min_{\beta} l(\beta)
$$
第 $t+1$​ 轮迭代解的更新公式：
$$
\beta ^{t+1} = \beta^t - \left( \frac{\partial^2{l(\beta)}}{\partial{\beta} \partial{\beta^T}} \right)^{-1} \frac{\partial{l(\beta)}}{\partial{\beta}}
$$
 其中 $l(\beta)$ 关于 $\beta$ 的一阶导  、二阶导的推导过程如下：

![一阶导](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404012206774.jpg)

![二阶导](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404012206214.jpg)

{% endfold %}

### 3.4 线性判别分析

pass

### 3.5 多分类学习

{% note info %}

我们一般采用多分类放集成的策略来解决多分类的学习任务。具体的学习任务大概是：将多分类任务拆分为多个二分类任务，每一个二分类任务训练一个学习器；在测试数据时，将所有的分类器进行集成以获得最终的分类结果。这里有两个关键点：如何拆分多分类任务？如何集成二分类学习器？本目主要介绍**多分类学习任务的拆分**。主要有三种拆分策略：一对多、一对其余、多对多。对于 N 个类别而言：

{% endnote %}

#### 3.5.1 一对一

{% fold info @图例 %}

![一对一](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404020850504.png)

{% endfold %}

- 名称：One vs. One，简称 OvO
- 训练：需要对 N 个类别进行 $\frac{N(N-1)}{2}$ 次训练，得到 $\frac{N(N-1)}{2}$ 个二分类学习器
- 测试：对于一个样本进行分类预测时，需要用 $\frac{N(N-1)}{2}$ 个学习器分别进行分类，最终分得的结果种类最多的类别就是样本的预测类别
- 特点：类别较少时，时间和内存开销往往更大；类别较多时，时间开销往往较小

#### 3.5.2 一对其余

{% fold info @图例 %}

![一对其余](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404020852608.png)

{% endfold %}

- 名称：One vs. Rest，简称 OvR
- 训练：需要对 N 个类别进行 $N$ 次训练，得到 $N$ 个二分类学习器。每次将目标类别作为正例，其余所有类别均为反例
- 测试：对于一个样本进行分类预测时，需要用 $N$ 个学习器分别进行分类，每一个学习器显然只会输出二值，假定为正负。正表示当前样例属于该学习器的正类，反之属于反类。若 $N$ 个学习器输出了多个正类标签，则还需通过执行度选择最终的类别。

#### 3.5.3 多对多

{% fold info @图例 %}

![多对多](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404020853931.png)

{% endfold %}

- 名称：Many vs. Many，简称 MvM
- 训练（编码）：对于 N 个类别数据，我们自定义 M 次划分。每次选择若干个类别作为正类，其余类作为反类。每一个样本在 M 个二分类学习器中都有一个分类结果，也就可以看做一个 M 维的向量。m 个数据也就构成了 m 个在 M 维空间的点阵。
- 测试（解码）：对于测试样本，对于 M 个学习器同样也会有 M 个类别标记构成的向量，我们计算当前样本与训练集构造的 m 个样本的海明距离、欧氏距离，距离当前测试样本对应的点属于的类别，我们认为就是当前测试样本的类别。

#### 3.5.4 softmax

类似于 M-P 神经元，目的是为了将当前测试样本属于各个类别的概率之和约束为 1。

### 3.6 类别不平衡问题

{% note info %}

在分类任务的数据集中，往往会出现类别不平衡的问题，即使在类别的样本数量相近时，在使用一对其余等算法进行多分类时，也会出现类比不平衡的问题，因此解决类比不平衡问题十分关键。

{% endnote %}

#### 3.6.1 阈值移动

**常规而言**，对于二分类任务。我们假设 $y$ 为样本属于正例的概率，则 $p=\frac{y}{1-y}$ 就是正确划分类别的概率。在假定类别数量相近时，我们用下式表示预测为正例的情况：
$$
\frac{y}{1-y}>1
$$
但是显然，**上述假设不总是成立**，我们令 $m^+$ 为样本正例数量，$m^-$ 为样本反例数量。我们用下式表示预测为正例的情况：
$$
\frac{y}{1-y} > \frac{m^+}{m^-}
$$
**根本初衷**是为了让 $\frac{m^+}{m^-}$ 表示数据类别的真实比例。但是由于训练数据往往不能遵循独立分布同分布原则，也就导致我们观测的 $\frac{m^+}{m^-}$ 其实不能准确代表数据的真实比例。那还有别的解决类别不平衡问题的策略吗？答案是有的！

#### 3.6.2 欠采样

即去除过多的样本使得正反例的数量近似，再进行学习。

- 优点：训练的时间开销小
- 缺点：可能会丢失重要信息

典型的算法是：EasyEnsemble

#### 3.6.3 过采样

即对训练集中类别数量较少的样本进行重复采样，再进行学习。

- 缺点：简单的重复采样会导致模型过拟合数据，缺少泛化能力。

典型的算法是：SMOTE

## 第4章 决策树

pass

## 第5章 神经网络

### 5.1 神经元模型

![M-P 神经元模型](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021447476.png)

我们介绍 M-P 神经元模型。该神经元模型必须具备以下三个特征：

1. 输入：来自其他连接的神经元传递过来的输入信号
2. 处理：输入信号通过带权重的连接进行传递，神经元接受到所有输入值的总和，再与神经元的阈值进行比较
3. 输出：通过激活函数的处理以得到输出

激活函数可以参考 3.3 中的逻辑函数（logistic function），此处将其声明为 sigmoid 函数，同样不采用不。连续光滑的分段函数。

### 5.2 感知机与多层网络

{% note info %}

本目从**无隐藏层的感知机**出发，介绍神经网络在简单的线性可分问题上的应用；接着介绍**含有一层隐藏层的多层感知机**，及其对于简单的非线性可分问题上的应用；最后引入多层前馈神经网络模型的概念。

{% endnote %}

#### 5.2.1 感知机

![感知机（Perceptron）](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021609339.png)

感知机（Perceptron）由两层神经元组成。第一层是输入层，第二层是输出层。其中只有输出层的神经元为功能神经元，也即 M-P 神经元。先不谈如何训练得到上面的 $w_1,w_2,\theta$，我们先看看上面的感知机训练出来以后可以有什么功能？

通过单层的感知机，我们可以实现简单的线性可分的分类任务，比如逻辑运算中的 **与、或、非** 运算，下面演示一下如何使用单层感知机实现上述三种逻辑运算：

{% fold light @使用单层感知机实现线性可分任务：与、或、非三种逻辑运算 %}

与运算、或运算是二维线性可分任务，一定可以找到一条直线将其划分为两个类别：

![二维线性可分任务](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404091955554.png)

非运算是一维线性可分任务，同样也可以找到一条直线将其划分为两个类别：

![一维线性可分任务](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404091959633.png)

{% endfold %}

#### 5.2.2 多层感知机

![神经网络图例：多层感知机](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021604059.png)

所谓的多层感知机其实就是增加了一个隐藏层，则神经网络模型就变为三层，含有一个输入层，一个隐藏层，和一个输出层，更准确的说应该是“单隐层网络”。其中隐藏层和输出层中的所有神经元均为功能神经元。

为了学习出网络中的连接权 $w_i$ 以及所有功能神经元中的阈值 $\theta_j$，我们需要通过每一次迭代的结果进行参数的修正，对于连接权 $w_i$ 而言，我们假设当前感知机的输出为 $\hat y$，则连接权 $w_i$ 应做以下调整。其中 $\eta$ 为学习率。
$$
\begin{aligned}
w_i \leftarrow w_i + \Delta w_i \\
\Delta_i = \eta (y - \hat y) x_i
\end{aligned}
$$
{% fold light @使用多层感知机实现异或逻辑运算 %}

![使用多层感知机实现异或逻辑运算](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092000730.png)

{% endfold %}

#### 5.2.3 多层前馈神经网络

![多层前馈神经网络结构示意图](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404021604208.png)

所谓多层前馈神经网络，定义就是各层神经元之间不会跨层连接，也不存在同层连接，其中：

- 输入层仅仅接受外界输入，没有函数处理功能
- 隐藏层和输出层进行函数处理

### 5.3 误差逆传播算法

{% note info %}

多层网络的学习能力比感知机的学习能力强很多。想要训练一个多层网络模型，仅仅通过感知机的参数学习规则是不够的，我们需要一个全新的、更强大的学习规则。这其中最优秀的就是误差逆传播算法（errorBackPropagation，简称 BP），往往用它来训练多层前馈神经网络。下面我们来了解一下 BP 算法的内容、参数推导与算法流程。

{% endnote %}

#### 5.3.1 模型参数

我们对着神经网络图，从输入到输出进行介绍与理解：

![单隐层神经网络](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404090923723.png)

- 隐层：对于隐层的第 $h$ 个神经元
    - 输入：$\alpha_h = \sum_{i=1}^dx_i v_{ih}$
    - 输出：$b_h = f(\alpha_h - \gamma_h)$
- 输出层：对于输出层的第 $j$ 个神经元
    - 输入：$\beta_j=\sum_{h=1}^q b_h w_{hj}$
    - 输出：$\hat y_j = f(\beta j - \theta_j)$

现在给定一个训练集学习一个分类器。其中每一个样本都含有 $d$ 个特征，$l$ 个输出。现在使用**标准 BP 神经网络模型**，每输入一个样本都迭代一次。对于单隐层神经网络而言，一共有 4 种参数，即：

- 输入层到隐层的 $d \times q$ 个权值 $v_{ih}(i=1,2,\cdots,d,\ h=1,2,\cdots,q)$
- 隐层的 $q$ 个 M-P 神经元的阈值 $\gamma_h(h=1,2,\cdots,q)$​
- 隐层到输出层的 $q\times l$ 个权值 $w_{hj}(h=1,2,\cdots,q,\ j=1,2,\cdots,l)$
- 输出层的 $l$ 个 M-P 神经元的阈值 $\theta_j(j=1,2,\cdots,l)$

#### 5.3.2 参数推导

确定损失函数。

- 对于上述 4 种参数，我们均采用梯度下降策略。**以损失函数的负梯度方向对参数进行调整**。每次输入一个训练样本，都会进行一次参数迭代更新，这叫**标准 BP 算法**。

- 根本目标是使损失函数尽可能小，我们定义损失函数 $E$ 为当前样本的均方误差，并为了求导计算方便添加一个常量 $\frac{1}{2}$，对于第 $k$ 个训练样本，有如下损失函数：

$$
E_k = \frac{1}{2} \sum _{j=1}^l (\hat y_j^k - y_j^k)^2
$$
确定迭代修正量。

- 假定当前学习率为 $\eta$，对于上述 4 种参数的迭代公式为：
    $$
    \begin{aligned}
    w_{hj} &\leftarrow w_{hj}+\Delta w_{hj} \\
    \theta_{j} &\leftarrow \theta_{j}+\Delta \theta_{j} \\
    v_{ih} &\leftarrow v_{ih}+\Delta v_{ih} \\
    \gamma_{h} &\leftarrow \gamma_{h}+\Delta \gamma_{h} \\
    \end{aligned}
    $$
    
- 其中，修正量分别为：
    $$
    \begin{aligned}
    \Delta w_{hj} &= \eta g_j b_h \\
    \Delta \theta_{j} &= -\eta g_j \\
    \Delta v_{ih} &= \eta e_h x_i \\
    \Delta \gamma_{h} &= -\eta e_h \\
    \end{aligned}
    $$

{% fold info @修正量推导 - 链式法则 %}

公式表示：

![公式表示](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092222942.jpg)

隐层到输出层的权重、输出神经元的阈值：

![隐层到输出层的权重、输出神经元的阈值](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092222625.jpg)

输入层到隐层的权重、隐层神经元的阈值：

![输入层到隐层的权重、隐层神经元的阈值](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404092223804.jpg)

{% endfold %}

#### 5.3.3 算法流程

对于当前样本的输出损失 $E_k$ 和学习率 $\eta$，我们进行以下迭代过程：

![BP 神经网络算法流程](https://dwj-oss.oss-cn-nanjing.aliyuncs.com/images/202404090920527.png)

还有一种 BP 神经网络方法就是**累计 BP 神经网络**算法，基本思路就是对于全局训练样本计算累计误差，从而更新参数。在实际应用过程中，一般先采用累计 BP 算法，再采用标准 BP 算法。还有一种思路就是使用随机 BP 算法，即每次随机选择一个训练样本进行参数更新。

### 5.4 全局最小与局部极小

pass

### 5.5 其他常见神经网络

pass

### 5.6 深度学习

pass

## 第6章 支持向量机

### 6.1 间隔与支持向量 TODO



### 6.2 对偶问题 TODO



### 6.3 核函数



### 6.4 软间隔与正则化



### 6.5 支持向量回归



### 6.6 核方法



## 第7章 贝叶斯分类



## 第8章 集成学习



## 第9章 聚类



## 第10章 降维与度量学习 *



## 第11章 特征选择与稀疏学习 *



## ~~第12章 计算学习理论~~



## 第13章 半监督学习



## 第14章 概率图模型



## ~~第15章 规则学习~~



## 第16章 强化学习

